# ğŸ“Œ GPT-2 From Scratch â€” Spam Classifier & Text Generator  

##  Project Overview  
This project implements **GPT-2 from scratch** using PyTorch, including all the core building blocks:  
- ğŸ§© Token embeddings  
- ğŸ“ Positional encoding layer  
- ğŸ”„ Transformer blocks (multi-head attention + feed-forward networks)  
- âš–ï¸ Layer normalization and residual connections  
- ğŸ“ Output classification/decoding head  

After building the architecture, I:  
1. **Downloaded the pretrained GPT-2 weights** released by OpenAI.  
2. **Fine-tuned the model** for two tasks:  
   - **Text generation** (creative continuation of input prompts).  
   - **Spam classification** (binary classification: SPAM vs HAM).  

The model achieves **100% accuracy** on the spam classification task, showing its ability to both generate coherent text and act as a robust classifier.  

## âœ¨ Features  

- âœ… **GPT-2 architecture** fully re-implemented from scratch  
- âœ… Supports **text generation** with autoregressive decoding  
- âœ… Supports **spam classification** with fine-tuning  
- âœ… Integrated with **pretrained OpenAI GPT-2 weights**  
- âœ… Easy to **retrain and extend** to new tasks  

## ğŸ“‚ Repository Structure  

```plaintext
â”œâ”€â”€ modules/                   # Core modules (attention, transformer blocks, etc.)  
â”œâ”€â”€ config/                    # Configurations and hyperparameters  
â”œâ”€â”€ data/                      # Dataset files (if applicable)  
â”œâ”€â”€ models                     # Folder to save the trained models 
â”œâ”€â”€ Figures                    # Folder to save the plotted figures 
â”œâ”€â”€ train.py                   # Script to retrain the GPT-2 model 
â”œâ”€â”€ main.py                    # the start point to the GPT-2 model  
â”œâ”€â”€ SpamClassifierTester.ipynb # Notebook to test the fine-tuned spam classifier  
â””â”€â”€ README.md                  # Project documentation  
